<!DOCTYPE html>
<html lang="en">
<head>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans&display=swap" rel="stylesheet">

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=0.7">
    <title>FindMe</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
   
    <div id="sidebar">
      <ul>
	<li><a href="../index.html">home</a></li>
	<li><a href="../projects.html">projects</a></li>
	<li><a href="../about.html">me!</a></li>
      </ul>
    </div>

    <div class="post">
        <h1>FindMe</h1>
	<div class="title-line"></div>


        <a href="https://github.com/jc-cr/FindMe" target="_blank">
            <img src="jap_hue.png" alt="GitHub Repo Image" class="border">
        </a>

	<h2>Overview</h2>
	<p><em> Click the image above to go to the Github Repo! </em></p>
	<p> A package for determining the relative direction of a voice and orienting a camera towards that voice.</p>


	<div class="section-line"></div>

	
	<h2>Project Notes</h2>
	
	
	<h3>Combining Azimuth Estimations and Keyword Spotting</h3>
	<div class="date">Apr 10, 2023</div>
	<div class="project-note-content">

	  <p>So at a high level for processign the audio, I plan on doing as follows: </p>

	  <div class="bullet-point">&#8226; <strong>Keyword spotting</strong>: Use one of the pretrained models from the ML-KWS-for-MCU repository to detect the "Hey robot" command in the audio stream. The keyword spotting model will output a confidence score indicating how likely it is that the command was spoken. You can set a threshold on this score to decide when the command is detected.</div>

	  <div class="bullet-point">&#8226; <strong>Audio buffering</strong>: While performing keyword spotting, store the incoming audio data in a circular buffer. The buffer should be large enough to hold the audio data corresponding to a small time window around the detected keyword.</div>

	  <div class="bullet-point">&#8226; <strong>Azimuth estimation on detection</strong>: When the keyword spotting model detects the "Hey robot" command, use the audio data in the buffer to perform azimuth estimation. Since you're only calculating azimuth when a keyword is detected, this will reduce the computational load.</div>

	  <p>By following this approach, you optimize the system by performing azimuth estimation only when necessary.</p>


	<h3> Getting Started </h3>
	<div class="date">Apr 10, 2023</div>
	<div class="project-note-content">
	  <p>So this project covers a few areas I am interested in exploring: making robots more intuitive to use and
	  work with, voice commands, and working with embedded hardware.</p>
	  <p>I'm still in the research phase of this project but here is the game plan thus far: </p>
	  <div class="bullet-point">&#8226; Setup Beagleboard Black (BBB) for audio input handling.</div>
	  <div class="bullet-point">&#8226; Implement a voice command filter so BBB can constantly listen but only
	    responds when it hears keywords or voice commands.</div>
	  <div class="bullet-point">&#8226; Implement software to decipher audio input and output an angle prediction for
	  where voice is coming from.</div>
	  <div class="bullet-point">&#8226; Translate audio prediction to servo movement.</div>	
	</div>

